{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed63ea45-f38d-4ba6-8515-47cb00955de2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### imports and custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a59da09-6248-4207-8bce-de8f19f4ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/gin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, KMeansSMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, MaxAbsScaler, StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFromModel, chi2, f_classif\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "import glob\n",
    "import difflib\n",
    "from thefuzz import process\n",
    "import pysrt\n",
    "import re\n",
    "PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17e9cf5-f585-4dc8-8dc5-928e06027eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML = r'<.*?>'\n",
    "TAG = r'{.*?}'\n",
    "COMMENTS = r'[\\(\\[][A-Z ]+[\\)\\]]'\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]'\n",
    "SPACES = r'([ ])\\1+'\n",
    "DOTS = r'[\\.]+'\n",
    "\n",
    "def clean_subs(subs):\n",
    "    txt = re.sub(HTML, ' ', subs) #html тэги меняем на пробел\n",
    "    txt = re.sub(TAG, ' ', txt) #тэги меняем на пробел\n",
    "    txt = re.sub(COMMENTS, ' ', txt) #комменты меняем на пробел\n",
    "    txt = re.sub(LETTERS, ' ', txt) #все что не буквы меняем на пробел\n",
    "    txt = re.sub(SPACES, r'\\1', txt) #повторяющиеся пробелы меняем на один пробел\n",
    "    txt = re.sub(DOTS, r'.', txt)  #многоточие меняем на точку\n",
    "    txt = txt.encode('ascii', 'ignore').decode() #удаляем все что не ascii символы   \n",
    "    txt = \".\".join(txt.lower().split('.')[1:-1]) #удаляем первый и последний субтитр (обычно это реклама)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5aac107-1dd6-4506-944c-bda7a0269676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ronakvijay/IMDB_Sentiment_Analysis/\n",
    "stop_words = stopwords.words('english') # defining stop_words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def data_preprocessing(review):\n",
    "    # data cleaning\n",
    "    # review = re.sub(re.compile('<.*?>'), '', review) #removing html tags\n",
    "    # review =  re.sub('[^A-Za-z0-9]+', ' ', review) #taking only words\n",
    "    # lowercase\n",
    "    review = review.lower()\n",
    "    # tokenization\n",
    "    tokens = nltk.word_tokenize(review) # converts review to tokens\n",
    "    # stop_words removal\n",
    "    review = [word for word in tokens if word not in stop_words] #removing stop words\n",
    "    # lemmatization\n",
    "    review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    # join words in preprocessed review\n",
    "    review = ' '.join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2cddcd-ab9d-4f3b-8949-42d10861749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_connect(df, path=PATH, cutoff=90):\n",
    "    subpath = path + 'subs/'\n",
    "    files = glob.glob(subpath+'*.srt') # list of all paths to subtitle files\n",
    "    file_names = list(map(lambda x: x.split('/')[-1], files)) # list of all subtitle file names\n",
    "    df['sub_file'] = df['Movie'].apply(lambda x: process.extractOne(x, file_names, score_cutoff=cutoff)) # levanshtein choices\n",
    "    df = df.dropna(axis=0) # drop rows without found subs\n",
    "    # df['path'] = df['path'].apply(lambda x: x[0])\n",
    "    \n",
    "    df.loc[:, 'sub'] = df.loc[:, 'sub_file'].apply(lambda x: (pysrt.open(subpath+x[0], encoding='iso-8859-1')).text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b3e4d88-c7cc-483f-a903-f49ced78be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fb2e2-2ffb-4a83-985c-d9324b04425f",
   "metadata": {},
   "source": [
    "#### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f04564-a6df-4465-a09a-ae7e40723216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('movies_labels.xlsx', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66cdec9-984c-4df6-b7b6-95e28a68d58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2            101\n",
       "B1             55\n",
       "C1             40\n",
       "A2/A2+         26\n",
       "B1, B2          8\n",
       "A2              6\n",
       "A2/A2+, B1      5\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f6b0600-8ad7-41ef-8395-f87e01c64d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here’s what you’re able to understand at each of these levels:\n",
    "\n",
    "#     A1 – I am able to recognize that someone is speaking this language, e.g. I recognize single words or phrases.\n",
    "#     A2 – I recognize a lot of words and phrases, but it’s hard to understand what is being said; I often only understand a phrase a few seconds after hearing it.\n",
    "#     B1 – I can understand more or less what is being said, but I still miss a lot.\n",
    "#     B2 – I can understand 90% of what is being said, but it’s hard for me to understand some actors, especially when they speak very quickly or with a specific accent, even though they say words that I know.\n",
    "#     C1 – I am able to watch a movie freely, but I miss some words and expressions. When I watch a comedy, not everything makes me laugh, because I do not understand many cultural references.\n",
    "#     C2 – I can understand virtually everything including cultural references and hidden meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7398923a-0e9e-419b-ab3a-5a4ba12b4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinning intermediate labels to the higher class\n",
    "df.loc[df['Level']=='A2/A2+, B1', 'Level'] = 'B1'\n",
    "df.loc[df['Level']=='A2/A2+', 'Level'] = 'A2'\n",
    "df.loc[df['Level']=='B1, B2', 'Level'] = 'B2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5385ef68-1792-46c9-a0c9-5b02d423072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding predicting class\n",
    "mapping = {value:key for key, value in enumerate(df['Level'].unique())}\n",
    "inv_mapping = {key:value for key, value in enumerate(df['Level'].unique())}\n",
    "\n",
    "df['Level'].replace(to_replace=mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b74a43-019c-45d9-a10f-7930ae0f16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_874703/3266688969.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'sub'] = df.loc[:, 'sub_file'].apply(lambda x: (pysrt.open(subpath+x[0], encoding='iso-8859-1')).text)\n"
     ]
    }
   ],
   "source": [
    "df = sub_connect(df)\n",
    "df.drop('sub_file', axis=1, inplace=True)\n",
    "\n",
    "df.loc[:, 'sub'] = df.loc[:, 'sub'].apply(clean_subs)\n",
    "df = df[df['sub'].apply(len) > 0]\n",
    "\n",
    "df['sub'] = df['sub'].apply(data_preprocessing)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4949c1fd-3676-499a-b44a-c71b1b86e323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"enjoy flick ben phone michelle , please n't hang . talk , okay ? ca n't believe left . michelle . come back . please say something . michelle , talk . look , argument . couple fight . reason leave everything behind . running away n't gon na help . michelle , please . newscaster detail . elsewhere today , power still restored many city southern seaboard wake afternoon 's widespread blackout . inclement weather region , problem seems linked authority calling catastrophic power surge crippled traffic area . . ! damn . okay . okay , please . please . please n't hurt . please . let go , okay ? wo n't tell anybody . promise , okay ? please let go . please . man need fluid . shock . going ? 'm going keep alive . work getting handy . boyfriend expecting . 'll send cop looking . 'm sorry . one looking . 've got fight . respect . n't even think trying . 're lucky . generosity extends far . egg . toradol help pain . please . please , let go . please . nowhere go , michelle . looked wallet . given\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 2][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25da21d1-3bc5-4227-bc8c-15637fd97aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['sub']\n",
    "y = df['Level']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y,\n",
    "                                                    random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b946ddc5-4feb-4a63-bef6-fe86ddf19d23",
   "metadata": {},
   "source": [
    "#### pipeline base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e4dd11-d8fe-4bfc-82bd-8b4502029b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[['count_vect', CountVectorizer(lowercase=True, min_df=2, max_df=50, ngram_range=(1, 3), decode_error='ignore')],\n",
    "                           ['tf_id', TfidfTransformer(use_idf=True)],\n",
    "                           ['sampling', SMOTE()],\n",
    "                           # ['poly', PolynomialFeatures(interaction_only=True)],\n",
    "                           # ['scaler', MaxAbsScaler()],\n",
    "                           # ['pca', TruncatedSVD(n_components=2048)],\n",
    "                           # ['feature_selection', SelectFromModel(LogisticRegression(class_weight='balanced', n_jobs=-1), max_features=500)],\n",
    "                           ['feature_selection', SelectKBest(chi2, k=5000)],\n",
    "                           # ['feature_selection', SelectPercentile(chi2, percentile=70)],\n",
    "                           # ['poly', PolynomialFeatures(degree=2, interaction_only=True)],\n",
    "                           # ['denser', Denser()],\n",
    "                           # ['scaler', StandardScaler()],\n",
    "                           # ['clf', LogisticRegression(n_jobs=-1)],\n",
    "                           # ['clf', PassiveAggressiveClassifier(C=1., class_weight='balanced', n_jobs=-1)],\n",
    "                           # ['clf', MultinomialNB()],\n",
    "                           # ['clf', XGBRFClassifier(n_estimators=500, booster='gblinear', n_jobs=-1)],                      \n",
    "                           # ['clf', LGBMClassifier(class_weight='balanced', learning_rate=0.1, n_estimators=100, max_depth=7000, n_jobs=-1)],\n",
    "                           # ['clf', GradientBoostingClassifier(n_estimators=100, max_depth=500, learning_rate=0.1)],\n",
    "                           ['clf', AdaBoostClassifier(base_estimator=LogisticRegression(n_jobs=-1), learning_rate=0.01, n_estimators=10)],\n",
    "                           # ['clf', AdaBoostClassifier(base_estimator=MultinomialNB(), learning_rate=0.1, n_estimators=10)],\n",
    "                           # ['clf', SVC(kernel='rbf', C=1)],\n",
    "                           # ['clf', RandomForestClassifier(n_jobs=-1)]\n",
    "                           # ['clf', BaggingClassifier(LogisticRegression(class_weight='balanced', n_jobs=-1))]\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d77b1468-665b-44e7-b5de-1db233b5c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cross_val_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=split, n_jobs=1)\n",
    "# cross_val_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c602acae-bc06-40a6-ad3c-9c943fe97c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy test score:  0.7272727272727273 \n",
      " f1score test score:  0.7193001443001443\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "print('accuracy test score: ', accuracy_score(y_test, preds), '\\n',\n",
    "      'f1score test score: ', f1_score(y_test, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b2ef9-f566-4173-afe2-2843d6f9cd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
